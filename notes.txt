REINFORCEMENT LEARNING (RL) GUIDE
==================================

1. INTRODUCTION
---------------
Reinforcement Learning is a type of machine learning where an agent learns to make 
decisions by interacting with an environment, receiving rewards or penalties for its 
actions, and gradually improving its behavior to maximize cumulative reward.

Key Characteristics:
- Learning through trial and error
- Sequential decision-making
- Delayed rewards
- Exploration vs. Exploitation trade-off


2. RL FRAMEWORK
--------------
Core Components:

Agent
- The learner and decision-maker
- Takes actions based on current state
- Learns from rewards/penalties
- Examples: AI playing chess, robot navigating maze

Environment
- The external system the agent interacts with
- Provides states and rewards
- Changes based on agent actions
- Examples: game board, physical world, simulation

State (s)
- Environment snapshot at a given time
- Contains all information needed to make decisions
- Can be fully observable (MDP) or partially observable (POMDP)
- Examples: position on game board, sensor readings

Action (a)
- Agent's choice in response to current state
- Can be discrete (up/down/left/right) or continuous (steering angle)
- Space of possible actions: action space A(s)

Reward (r)
- Feedback signal for agent's action
- Immediate feedback (not delayed evaluation)
- Scalar value (positive = good, negative = bad)
- Goal: maximize cumulative reward (return)

Policy (π)
- Agent's strategy: π(a|s) = probability of taking action a in state s
- Maps states to actions
- Can be deterministic or stochastic
- This is what the agent learns/optimizes

Value Functions
- V^π(s): Expected cumulative reward from state s following policy π
- Q^π(s,a): Expected cumulative reward from state s taking action a, then following π
- Help estimate long-term value of states/actions


3. MATHEMATICAL FRAMEWORK
-------------------------
Markov Decision Process (MDP):
- States: S
- Actions: A
- Transition probabilities: P(s'|s,a)
- Reward function: R(s,a,s')
- Discount factor: γ (0 ≤ γ ≤ 1)

Return (G_t):
- Total discounted reward: G_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ...
- γ < 1: values future rewards less
- γ = 1: equal weight to all future rewards
- γ = 0: only cares about immediate reward

Bellman Equations:
- V^π(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a)[R(s,a,s') + γV^π(s')]
- Q^π(s,a) = Σ_{s'} P(s'|s,a)[R(s,a,s') + γΣ_{a'} π(a'|s')Q^π(s',a')]

Optimal Policy (π*):
- Policy that maximizes expected return from all states
- π* = argmax_π V^π(s) for all s


4. KEY ALGORITHMS
-----------------
Value-Based Methods:
- Learn value function V(s) or Q(s,a)
- Policy derived from value function (e.g., choose best action)
- Examples: Q-Learning, Deep Q-Network (DQN), SARSA

Policy-Based Methods:
- Directly learn policy π(a|s)
- No explicit value function
- Examples: REINFORCE, Policy Gradient, Actor-Critic

Model-Based Methods:
- Learn model of environment (transition probabilities, rewards)
- Plan using learned model
- Examples: Dyna-Q, AlphaZero (hybrid)

Hybrid Methods:
- Combine value and policy learning
- Examples: Actor-Critic, PPO (Proximal Policy Optimization), A3C


5. Q-LEARNING (Example Algorithm)
---------------------------------
Off-policy temporal difference (TD) learning algorithm.

Key Concept:
- Learn Q*(s,a) = optimal action-value function
- Q*(s,a) = E[G_t | S_t=s, A_t=a, following optimal policy]
- Use Bellman equation: Q(s,a) ← r + γ max_{a'} Q(s',a')

Algorithm:
1. Initialize Q(s,a) arbitrarily
2. For each episode:
   a. Initialize state s
   b. Repeat until terminal:
      - Choose action a using ε-greedy policy (exploration)
      - Take action a, observe reward r, next state s'
      - Update: Q(s,a) ← Q(s,a) + α[r + γ max_{a'} Q(s',a') - Q(s,a)]
      - s ← s'

ε-Greedy Exploration:
- With probability ε: choose random action (explore)
- With probability 1-ε: choose best action argmax_a Q(s,a) (exploit)
- Usually ε decreases over time (annealing)


6. DEEP REINFORCEMENT LEARNING
------------------------------
Problem: Q-Learning doesn't scale to large state spaces (e.g., images)

Solution: Use neural networks as function approximators
- DQN: Deep Q-Network approximates Q(s,a)
- Input: state (e.g., image pixels)
- Output: Q-values for each action

Key Techniques:
- Experience Replay: Store transitions (s,a,r,s') in buffer, sample batches
- Target Network: Separate network for computing target Q-values (stability)
- Gradient Clipping: Prevent exploding gradients

Famous Example: DQN playing Atari games from raw pixels


7. GYMNASIUM (OpenAI Gym) - PRACTICAL IMPLEMENTATION
-----------------------------------------------------
Gymnasium provides standardized RL environments.

Basic Usage:
```python
import gymnasium as gym

# Create environment
env = gym.make('CartPole-v1')

# Reset environment
observation, info = env.reset()

# Agent loop
for step in range(1000):
    # Agent chooses action (random example)
    action = env.action_space.sample()
    
    # Take step
    observation, reward, terminated, truncated, info = env.step(action)
    
    # Check if episode ended
    if terminated or truncated:
        observation, info = env.reset()

env.close()
```

Environment Interface:
- env.reset(): Reset to initial state, return initial observation
- env.step(action): Execute action, return:
  * observation: next state
  * reward: scalar reward
  * terminated: episode ended (success/failure)
  * truncated: episode ended (time limit)
  * info: additional debugging information

Spaces:
- env.observation_space: Defines valid observations
- env.action_space: Defines valid actions

Popular Environments:
- Classic Control: CartPole-v1, MountainCar-v0, Acrobot-v1
- Atari: Breakout-v4, Pong-v4, SpaceInvaders-v4
- Box2D: LunarLander-v2, BipedalWalker-v3
- MuJoCo: Humanoid-v4, Ant-v4


8. IMPLEMENTING Q-LEARNING WITH GYMNASIUM
------------------------------------------
```python
import gymnasium as gym
import numpy as np

class QLearningAgent:
    def __init__(self, env, learning_rate=0.1, discount=0.99, epsilon=1.0):
        self.env = env
        self.lr = learning_rate
        self.gamma = discount
        self.epsilon = epsilon
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        
        # Discrete action/state spaces needed
        # For continuous states, use discretization or function approximation
        n_states = env.observation_space.n  # For Discrete state space
        n_actions = env.action_space.n
        self.q_table = np.zeros((n_states, n_actions))
    
    def choose_action(self, state, training=True):
        if training and np.random.random() < self.epsilon:
            return self.env.action_space.sample()  # Explore
        return np.argmax(self.q_table[state])  # Exploit
    
    def update_q(self, state, action, reward, next_state, done):
        current_q = self.q_table[state, action]
        if done:
            target_q = reward
        else:
            target_q = reward + self.gamma * np.max(self.q_table[next_state])
        
        self.q_table[state, action] += self.lr * (target_q - current_q)
        
        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Example usage with FrozenLake
env = gym.make('FrozenLake-v1', is_slippery=True)
agent = QLearningAgent(env)

for episode in range(1000):
    state, info = env.reset()
    done = False
    
    while not done:
        action = agent.choose_action(state)
        next_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        
        agent.update_q(state, action, reward, next_state, done)
        state = next_state

env.close()
```


9. EXPLORATION VS. EXPLOITATION
-------------------------------
Fundamental trade-off in RL:

Exploitation:
- Use current knowledge to choose best action
- Maximize immediate reward
- Risk: miss better long-term strategies

Exploration:
- Try new actions to discover better policies
- Learn more about environment
- Risk: suboptimal immediate performance

Strategies:
- ε-greedy: Random exploration with probability ε
- Softmax/Boltzmann: Actions sampled based on Q-values (probability)
- UCB (Upper Confidence Bound): Optimistic about uncertain actions
- Thompson Sampling: Bayesian approach
- Intrinsic Motivation: Reward for novelty/uncertainty


10. COMMON CHALLENGES & SOLUTIONS
----------------------------------
Sparse Rewards:
- Problem: Agent rarely receives reward signal
- Solutions: Reward shaping, curriculum learning, hierarchical RL

Credit Assignment:
- Problem: Which actions led to reward?
- Solutions: Temporal difference learning, eligibility traces

Non-stationary Environments:
- Problem: Environment changes over time
- Solutions: Continual learning, meta-learning

Sample Efficiency:
- Problem: Requires many samples to learn
- Solutions: Experience replay, prioritized replay, better function approximation

Stability:
- Problem: Learning can be unstable
- Solutions: Target networks, gradient clipping, careful hyperparameter tuning


11. EVALUATION METRICS
----------------------
- Episode Return: Sum of rewards in an episode
- Average Return: Mean return over multiple episodes
- Success Rate: Percentage of episodes ending in success
- Sample Efficiency: Episodes/samples needed to reach performance
- Convergence: How quickly algorithm reaches optimal policy


12. BEST PRACTICES
------------------
1. Start Simple: Begin with simple environments (CartPole, FrozenLake)
2. Understand the Environment: Know observation/action spaces, reward structure
3. Hyperparameter Tuning: Learning rate, discount factor, exploration rate critical
4. Visualize: Plot learning curves, watch agent play
5. Debugging: Print rewards, Q-values, check for NaN/inf
6. Reproducibility: Set random seeds for env and agent
7. Gradual Complexity: Move to harder environments progressively
8. Use Libraries: Leverage stable-baselines3, Ray RLlib for advanced algorithms


13. NEXT STEPS
--------------
- Learn advanced algorithms: PPO, SAC, TD3
- Deep RL: DQN, DDPG, A3C
- Continuous action spaces: Policy gradient methods
- Multi-agent RL
- Transfer learning in RL
- Real-world applications: robotics, game playing, recommendation systems


14. RESOURCES
-------------
- Gymnasium Documentation: https://gymnasium.farama.org/
- Sutton & Barto: "Reinforcement Learning: An Introduction" (textbook)
- Spinning Up in Deep RL: OpenAI's educational resource
- Stable-Baselines3: RL algorithms library
- Papers with Code: Latest RL research papers
